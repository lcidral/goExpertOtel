# Arquitetura do Sistema - GoExpertOtel

Este documento descreve a arquitetura detalhada do sistema de temperatura por CEP com OpenTelemetry.

## ğŸ—ï¸ VisÃ£o Geral da Arquitetura

O sistema implementa uma arquitetura de microserviÃ§os distribuÃ­da com observabilidade completa:

![archtecture.png](archtecture.png)

## ğŸ”§ Componentes do Sistema

### Service A - Input Service
**Responsabilidade:** Ponto de entrada do sistema, validaÃ§Ã£o de CEP

**Porta:** 8080

**Funcionalidades:**
- Recebe requisiÃ§Ãµes HTTP POST com CEP
- Valida formato do CEP (8 dÃ­gitos numÃ©ricos)
- Encaminha para Service B via HTTP
- Retorna respostas formatadas ao cliente

**Endpoints:**
- `POST /` - Processa CEP
- `GET /health` - Health check

**Fluxo de Processamento:**
1. Recebe JSON `{"cep": "12345678"}`
2. Valida formato usando `pkg/utils/cep_validator.go`
3. Chama Service B via client HTTP
4. Retorna resposta ou erro apropriado

### Service B - Temperature Orchestrator
**Responsabilidade:** OrquestraÃ§Ã£o de busca de localizaÃ§Ã£o e temperatura

**Porta:** 8081

**Funcionalidades:**
- Revalida CEP recebido
- Busca localizaÃ§Ã£o via OpenCEP API
- Consulta temperatura via WeatherAPI
- Converte temperaturas (CÂ°, FÂ°, KÂ°)
- Implementa cache inteligente
- Retorna dados consolidados

**Endpoints:**
- `POST /temperature` - Busca temperatura (interno)
- `GET /health` - Health check com cache stats
- `GET /cache/stats` - EstatÃ­sticas de cache

**Fluxo de Processamento:**
1. Recebe CEP do Service A
2. Verifica cache de resposta completa
3. Se nÃ£o em cache:
   - Busca localizaÃ§Ã£o (com cache de 24h)
   - Busca temperatura (com cache de 10min)
   - Converte temperaturas matematicamente
   - Armazena resposta em cache (10min)
4. Retorna resultado

### OpenTelemetry Collector
**Responsabilidade:** Coleta, processa e exporta traces

**Portas:** 4317 (gRPC), 4318 (HTTP)

**ConfiguraÃ§Ã£o:**
- Recebe traces via protocolo OTLP
- Processa em batches para eficiÃªncia
- Adiciona atributos de ambiente
- Exporta para Zipkin e logs

**Pipeline:**
```
Traces â†’ Batch Processor â†’ Attributes Processor â†’ Zipkin Exporter
```

### Zipkin
**Responsabilidade:** Interface de visualizaÃ§Ã£o de traces

**Porta:** 9411

**Funcionalidades:**
- UI web para exploraÃ§Ã£o de traces
- Busca por serviÃ§o, operaÃ§Ã£o, tags
- AnÃ¡lise de latÃªncia e dependÃªncias
- Storage em memÃ³ria (desenvolvimento)

## ğŸ“Š Fluxo de Dados Completo

### RequisiÃ§Ã£o de Sucesso

![sequence-diagram.png](sequence-diagram.png)

### Spans DistribuÃ­dos

**Service A Spans:**
1. `http.request` - Span raiz da requisiÃ§Ã£o HTTP
   - Atributos: method, url, status_code
2. `cep.validation` - ValidaÃ§Ã£o do CEP
   - Atributos: cep.input, cep.normalized, cep.valid
3. `service_b.call` - Chamada para Service B
   - Atributos: cep.value, service.name, response data

**Service B Spans:**
1. `http.request` - Span raiz da requisiÃ§Ã£o HTTP
2. `cep.validation` - RevalidaÃ§Ã£o do CEP
3. `cache.lookup` - Busca no cache
   - Atributos: cache.key, cache.type, cache.hit
4. `opencep.api.call` - Chamada para OpenCEP
   - Atributos: cep.value, api.name, city.name
5. `weather.api.call` - Chamada para WeatherAPI
   - Atributos: location, api.name, temp_c
6. `temperature.conversion` - ConversÃµes matemÃ¡ticas
   - Atributos: temp_c, temp_f, temp_k
7. `cache.store` - Armazenamento no cache
   - Atributos: cache.key, cache.type, cache.ttl

## ğŸ›ï¸ Estrutura do Monorepo

### OrganizaÃ§Ã£o de CÃ³digo

```
goExpertOtel/
â”œâ”€â”€ services/                    # MicroserviÃ§os independentes
â”‚   â”œâ”€â”€ service-a/
â”‚   â”‚   â”œâ”€â”€ cmd/server/         # Ponto de entrada
â”‚   â”‚   â”œâ”€â”€ internal/           # CÃ³digo privado do serviÃ§o
â”‚   â”‚   â”‚   â”œâ”€â”€ handler/        # HTTP handlers
â”‚   â”‚   â”‚   â””â”€â”€ client/         # Cliente para Service B
â”‚   â”‚   â””â”€â”€ config/             # ConfiguraÃ§Ãµes especÃ­ficas
â”‚   â””â”€â”€ service-b/
â”‚       â”œâ”€â”€ cmd/server/         # Ponto de entrada
â”‚       â”œâ”€â”€ internal/           # CÃ³digo privado do serviÃ§o
â”‚       â”‚   â”œâ”€â”€ handler/        # HTTP handlers
â”‚       â”‚   â”œâ”€â”€ client/         # Clientes APIs externas
â”‚       â”‚   â”œâ”€â”€ cache/          # Sistema de cache
â”‚       â”‚   â”œâ”€â”€ service/        # LÃ³gica de negÃ³cio
â”‚       â”‚   â””â”€â”€ model/          # Modelos internos
â”‚       â””â”€â”€ config/             # ConfiguraÃ§Ãµes especÃ­ficas
â”œâ”€â”€ pkg/                        # CÃ³digo compartilhado
â”‚   â”œâ”€â”€ models/                 # Modelos de dados compartilhados
â”‚   â”œâ”€â”€ telemetry/             # OpenTelemetry compartilhado
â”‚   â””â”€â”€ utils/                  # UtilitÃ¡rios compartilhados
â”œâ”€â”€ deployments/               # Infraestrutura e Docker
â”‚   â”œâ”€â”€ docker/                # Docker Compose e Dockerfiles
â”‚   â””â”€â”€ otel-collector/        # ConfiguraÃ§Ã£o OTEL Collector
â””â”€â”€ docs/                      # DocumentaÃ§Ã£o do projeto
```

### PrincÃ­pios de Design

1. **Separation of Concerns**: Cada serviÃ§o tem responsabilidade bem definida
2. **Shared Code**: CÃ³digo comum em `pkg/` para evitar duplicaÃ§Ã£o
3. **Clean Architecture**: DependÃªncias apontam sempre para dentro
4. **Observability First**: Telemetria Ã© cidadÃ£ de primeira classe
5. **Fail-Safe**: Sistema resiliente com fallbacks e timeouts

## ğŸ”„ EstratÃ©gia de Cache

### Multi-Layer Caching

**Layer 1 - Cache de LocalizaÃ§Ã£o (24h TTL)**
- Chave: `location:{cep}`
- Dados: Resposta completa do OpenCEP
- Justificativa: LocalizaÃ§Ã£o nÃ£o muda

**Layer 2 - Cache de Temperatura (10min TTL)**
- Chave: `weather:{cidade,estado}`
- Dados: Resposta da WeatherAPI
- Justificativa: Dados meteorolÃ³gicos mudam rapidamente

**Layer 3 - Cache de Resposta (10min TTL)**
- Chave: `temp:{cep}`
- Dados: Resposta final processada
- Justificativa: Evita reprocessamento desnecessÃ¡rio

### Cache Strategy Benefits

1. **Performance**: Reduz latÃªncia de ~2s para ~50ms
2. **Resilience**: Sistema funciona mesmo com APIs externas instÃ¡veis
3. **Cost**: Reduz nÃºmero de chamadas para APIs pagas
4. **User Experience**: Respostas mais rÃ¡pidas para CEPs populares

## ğŸš¦ PadrÃµes de Erro e ResilÃªncia

### CÃ³digos de Resposta HTTP

| CÃ³digo | CenÃ¡rio | Exemplo |
|--------|---------|---------|
| 200 | Sucesso | CEP vÃ¡lido e temperatura encontrada |
| 404 | CEP nÃ£o encontrado | OpenCEP retorna CEP inexistente |
| 422 | CEP invÃ¡lido | Formato incorreto (nÃ£o 8 dÃ­gitos) |
| 500 | Erro interno | APIs externas indisponÃ­veis |

### Tratamento de Falhas

1. **Validation Errors**: Detectados cedo no Service A
2. **Network Timeouts**: 10s para APIs externas, 30s entre serviÃ§os
3. **API Failures**: Logs detalhados + spans marcados com erro
4. **Cache Misses**: DegradaÃ§Ã£o graceful, nÃ£o falha crÃ­tica

### Circuit Breaker Pattern

Implementado via timeouts e retry policies:
- Timeout de 10s para APIs externas
- Timeout de 30s para comunicaÃ§Ã£o entre serviÃ§os
- Logs estruturados para debugging

## ğŸ” Observabilidade e Monitoramento

### MÃ©tricas Importantes

1. **LatÃªncia por Endpoint**
   - Service A: POST / (~100ms tÃ­pico)
   - Service B: POST /temperature (~500ms sem cache)

2. **Cache Hit Rates**
   - Location cache: ~90% em produÃ§Ã£o
   - Weather cache: ~70% em produÃ§Ã£o
   - Response cache: ~80% em produÃ§Ã£o

3. **Error Rates**
   - CEP invÃ¡lido: ~15% das requisiÃ§Ãµes
   - CEP nÃ£o encontrado: ~5% das requisiÃ§Ãµes
   - API failures: <1% das requisiÃ§Ãµes

### Traces Ãšteis para Debug

1. **Trace ID Propagation**: Permite seguir requisiÃ§Ã£o end-to-end
2. **Span Timing**: Identifica gargalos de performance
3. **Error Spans**: Mostra exatamente onde falhas ocorrem
4. **Cache Spans**: Permite otimizar estratÃ©gia de cache

## ğŸ” ConsideraÃ§Ãµes de SeguranÃ§a

### API Keys
- WeatherAPI key configurada via variÃ¡vel de ambiente
- NÃ£o logada ou exposta em traces
- RotaÃ§Ã£o recomendada mensalmente

### Network Security
- ComunicaÃ§Ã£o entre serviÃ§os via rede Docker isolada
- APIs externas via HTTPS quando possÃ­vel
- Rate limiting implÃ­cito via cache

### Data Privacy
- CEPs sÃ£o dados pÃºblicos, sem restriÃ§Ãµes
- Temperaturas sÃ£o dados pÃºblicos
- Logs nÃ£o contÃªm informaÃ§Ãµes sensÃ­veis

## ğŸš€ Performance e Escalabilidade

### Benchmarks Atuais
- Service A: ~1000 RPS sustentado
- Service B: ~500 RPS sustentado
- LatÃªncia P95: <200ms com cache hit
- LatÃªncia P95: <2s com cache miss

### Scaling Strategy
1. **Horizontal Scaling**: Cada serviÃ§o pode escalar independentemente
2. **Cache Optimization**: Aumentar TTL em produÃ§Ã£o
3. **Database**: Migrar cache para Redis em produÃ§Ã£o
4. **Load Balancing**: Nginx ou cloud load balancer

### Bottlenecks Identificados
1. **External APIs**: OpenCEP e WeatherAPI sÃ£o limitantes
2. **Memory Cache**: Limitado a memÃ³ria do container
3. **Single Instance**: NÃ£o hÃ¡ redundÃ¢ncia atualmente

## ğŸ”® EvoluÃ§Ã£o Futura

### PrÃ³ximas Features
1. **Metrics**: Prometheus + Grafana
2. **Alerting**: Alertas baseados em SLIs/SLOs
3. **Database**: Cache persistente com Redis
4. **Authentication**: API keys para clientes
5. **Rate Limiting**: ProteÃ§Ã£o contra abuso

### Architectural Evolution
1. **Event-Driven**: Migrar para eventos assÃ­ncronos
2. **Microservices**: Separar busca de localizaÃ§Ã£o e temperatura
3. **CQRS**: Separar leitura e escrita
4. **Service Mesh**: Istio para comunicaÃ§Ã£o entre serviÃ§os

---

Este documento serve como referÃªncia arquitetural e deve ser atualizado conforme o sistema evolui.